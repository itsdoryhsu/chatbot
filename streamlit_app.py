#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import uuid
import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    TextLoader,
    UnstructuredURLLoader
)
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate
import re
import pandas as pd
from datetime import datetime
import yt_dlp
import requests

# è¨­ç½®é é¢é…ç½®
st.set_page_config(
    page_title="è²¡å‹™ç¨…æ³•QAæ©Ÿå™¨äºº",
    page_icon="ğŸ’¼",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ–ç›®éŒ„
os.makedirs("data/documents", exist_ok=True)
os.makedirs("data/youtube", exist_ok=True)
os.makedirs("data/vectorstore", exist_ok=True)

# åˆå§‹åŒ–æœƒè©±ç‹€æ…‹
if "conversation" not in st.session_state:
    st.session_state.conversation = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "document_list" not in st.session_state:
    st.session_state.document_list = []
if "openai_api_key" not in st.session_state:
    st.session_state.openai_api_key = ""
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "selected_model" not in st.session_state:
    st.session_state.selected_model = "gpt-3.5-turbo-16k"
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è²¡å‹™ç¨…æ³•é¡§å•ï¼Œè² è²¬å›ç­”ç”¨æˆ¶çš„è²¡å‹™å’Œç¨…æ³•å•é¡Œã€‚

è«‹éµå¾ªä»¥ä¸‹æŒ‡å°åŸå‰‡ï¼š
1. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”æ‰€æœ‰å•é¡Œï¼Œå³ä½¿ç”¨æˆ¶ä½¿ç”¨ç°¡é«”ä¸­æ–‡æå•ã€‚
2. é¦–å…ˆä»”ç´°åˆ†æç”¨æˆ¶å•é¡Œçš„çœŸæ­£æ„åœ–å’Œèªç¾©ï¼Œç†è§£ç”¨æˆ¶çœŸæ­£æƒ³çŸ¥é“çš„æ˜¯ä»€éº¼ã€‚
3. åŸºæ–¼æä¾›çš„æ–‡æª”å…§å®¹å›ç­”å•é¡Œï¼Œä½†ä¸è¦åƒ…åƒ…è¤‡è£½æ–‡æª”ä¸­çš„å…§å®¹ã€‚
4. å¦‚æœæ–‡æª”ä¸­çš„ä¿¡æ¯ä¸å®Œæ•´ï¼Œè«‹ä½¿ç”¨ä½ çš„å°ˆæ¥­çŸ¥è­˜è£œå……å›ç­”ï¼Œä½†æ˜ç¢ºå€åˆ†å“ªäº›æ˜¯ä¾†è‡ªæ–‡æª”çš„ä¿¡æ¯ï¼Œå“ªäº›æ˜¯ä½ çš„å°ˆæ¥­è£œå……ã€‚
5. å¦‚æœæ–‡æª”ä¸­å®Œå…¨æ²’æœ‰ç›¸é—œä¿¡æ¯ï¼Œè«‹èª å¯¦åœ°èªªæ˜ï¼Œä¸¦æä¾›ä½ çš„å°ˆæ¥­å»ºè­°æˆ–å¼•å°ç”¨æˆ¶å°‹æ‰¾æ›´å¤šè³‡æºã€‚
6. å›ç­”æ‡‰è©²å°ˆæ¥­ã€æº–ç¢ºã€æ˜“æ–¼ç†è§£ï¼Œä¸¦å¼•ç”¨ç›¸é—œçš„æ³•è¦æˆ–æ–‡æª”ä¾†æºã€‚
7. å°æ–¼æœƒè¨ˆã€ç¨…å‹™ç­‰å°ˆæ¥­å•é¡Œï¼Œè«‹æä¾›ç³»çµ±æ€§çš„å›ç­”ï¼Œè€Œä¸åƒ…åƒ…æ˜¯åˆ—å‡ºæ–‡æª”ä¸­æåˆ°çš„ç‰‡æ®µã€‚

è¨˜ä½ï¼šä½ çš„ç›®æ¨™æ˜¯çœŸæ­£è§£æ±ºç”¨æˆ¶çš„å•é¡Œï¼Œè€Œä¸åƒ…åƒ…æ˜¯æª¢ç´¢å’Œå‘ˆç¾æ–‡æª”å…§å®¹ã€‚"""

# å´é‚Šæ¬„ - APIè¨­ç½®
with st.sidebar:
    st.title("ğŸ’¼ è²¡å‹™ç¨…æ³•QAæ©Ÿå™¨äºº")
    
    # APIå¯†é‘°è¼¸å…¥
    api_key = st.text_input("OpenAI API Key", type="password")
    if api_key:
        st.session_state.openai_api_key = api_key
        os.environ["OPENAI_API_KEY"] = api_key
    
    # æ¨¡å‹é¸æ“‡
    st.subheader("æ¨¡å‹é¸æ“‡")
    model_options = {
        "GPT-3.5 Turbo": "gpt-3.5-turbo-16k",
        "GPT-4o": "gpt-4o",
        "GPT-4.1 Turbo": "gpt-4-turbo",
        "GPT-4 Mini": "gpt-4-mini"
    }
    selected_model_name = st.selectbox(
        "é¸æ“‡OpenAIæ¨¡å‹",
        options=list(model_options.keys()),
        index=0
    )
    st.session_state.selected_model = model_options[selected_model_name]
    st.caption(f"ç•¶å‰é¸æ“‡: {st.session_state.selected_model}")
    
    st.divider()
    
    # ç³»çµ±æç¤ºè¨­ç½®
    st.subheader("ç³»çµ±æç¤ºè¨­ç½®")
    system_prompt = st.text_area(
        "è‡ªå®šç¾©ç³»çµ±æç¤º",
        value=st.session_state.system_prompt,
        height=150,
        help="é€™æ˜¯çµ¦AIçš„æŒ‡ä»¤ï¼Œå‘Šè¨´å®ƒå¦‚ä½•å›ç­”å•é¡Œã€‚æ‚¨å¯ä»¥æ ¹æ“šéœ€è¦ä¿®æ”¹ã€‚"
    )
    st.session_state.system_prompt = system_prompt
    
    # é¸æ“‡é é¢
    st.divider()
    st.subheader("å°èˆª")
    page = st.radio("é¸æ“‡é é¢", ["èŠå¤©å°è©±", "çŸ¥è­˜åº«ç®¡ç†"])

# å·¥å…·å‡½æ•¸
def extract_youtube_id(url):
    """å¾YouTube URLä¸­æå–è¦–é »ID"""
    youtube_regex = r"(?:youtube\.com\/(?:[^\/\n\s]+\/\S+\/|(?:v|e(?:mbed)?)\/|\S*?[?&]v=)|youtu\.be\/)([a-zA-Z0-9_-]{11})"
    match = re.search(youtube_regex, url)
    return match.group(1) if match else None

def get_youtube_transcript(youtube_id):
    """ç²å–YouTubeè¦–é »çš„å­—å¹•"""
    try:
        youtube_url = f"https://www.youtube.com/watch?v={youtube_id}"
        
        # ä½¿ç”¨yt_dlpç²å–è¦–é »ä¿¡æ¯
        with st.spinner("æ­£åœ¨ç²å–YouTubeå­—å¹•..."):
            with yt_dlp.YoutubeDL({}) as ydl:
                info = ydl.extract_info(youtube_url, download=False)
                subs = info.get('subtitles', {})
                auto_subs = info.get('automatic_captions', {})
                
                # é¡¯ç¤ºå¯ç”¨çš„å­—å¹•èªè¨€
                all_langs = list(subs.keys()) + list(auto_subs.keys())
                if all_langs:
                    st.info(f"è©²è¦–é »æœ‰ä»¥ä¸‹èªè¨€çš„å­—å¹•å¯ç”¨: {', '.join(all_langs)}")
                
                # å„ªå…ˆæ‰‹å‹•å­—å¹•ï¼Œå…¶æ¬¡è‡ªå‹•å­—å¹•
                target = None
                source_type = None
                lang_found = None
                
                # å„ªå…ˆæŸ¥æ‰¾ä¸­æ–‡å­—å¹•
                for source, source_name in [(subs, "æ‰‹å‹•"), (auto_subs, "è‡ªå‹•")]:
                    for lang, tracks in source.items():
                        if lang.startswith('zh'):
                            # æ‰¾åˆ°å­—å¹•ç¶²å€
                            for track in tracks:
                                if track['ext'] == 'vtt':
                                    target = track['url']
                                    source_type = source_name
                                    lang_found = lang
                                    break
                        if target:
                            break
                    if target:
                        break
                
                # å¦‚æœæ²’æœ‰ä¸­æ–‡å­—å¹•ï¼Œå˜—è©¦è‹±æ–‡å­—å¹•
                if not target:
                    for source, source_name in [(subs, "æ‰‹å‹•"), (auto_subs, "è‡ªå‹•")]:
                        for lang, tracks in source.items():
                            if lang.startswith('en'):
                                # æ‰¾åˆ°å­—å¹•ç¶²å€
                                for track in tracks:
                                    if track['ext'] == 'vtt':
                                        target = track['url']
                                        source_type = source_name
                                        lang_found = lang
                                        break
                            if target:
                                break
                        if target:
                            break
                
                # å¦‚æœé‚„æ˜¯æ²’æœ‰ï¼Œå˜—è©¦ä»»ä½•å¯ç”¨çš„å­—å¹•
                if not target and (subs or auto_subs):
                    for source, source_name in [(subs, "æ‰‹å‹•"), (auto_subs, "è‡ªå‹•")]:
                        if source:
                            first_lang = list(source.keys())[0]
                            tracks = source[first_lang]
                            for track in tracks:
                                if track['ext'] == 'vtt':
                                    target = track['url']
                                    source_type = source_name
                                    lang_found = first_lang
                                    break
                            if target:
                                break
                
                if not target:
                    st.error("æ­¤å½±ç‰‡æ²’æœ‰ä»»ä½•å¯ç”¨å­—å¹•")
                    return None
                
                st.success(f"æ‰¾åˆ°{lang_found}å­—å¹•ï¼Œä¾†æºï¼š{source_type}")
                
                # ä¸‹è¼‰å­—å¹•å…§å®¹
                r = requests.get(target)
                
                # è§£æVTTæ ¼å¼å­—å¹•
                transcript = []
                for line in r.text.splitlines():
                    # éæ¿¾æ‰VTTæ¨™é ­ã€æ™‚é–“è»¸ç­‰
                    if line.strip() == "" or "-->" in line or line.startswith("WEBVTT"):
                        continue
                    transcript.append(line.strip())
                
                # åˆä½µå­—å¹•æ–‡æœ¬
                return " ".join(transcript)
    except Exception as e:
        st.error(f"ç²å–YouTubeå­—å¹•å¤±æ•—: {str(e)}")
        return None

def get_youtube_info(youtube_id):
    """ç²å–YouTubeè¦–é »çš„ä¿¡æ¯"""
    try:
        youtube_url = f"https://www.youtube.com/watch?v={youtube_id}"
        
        # ä½¿ç”¨yt_dlpç²å–è¦–é »ä¿¡æ¯
        with yt_dlp.YoutubeDL({}) as ydl:
            info = ydl.extract_info(youtube_url, download=False)
            
            return {
                "title": info.get('title', f"YouTubeè¦–é » {youtube_id}"),
                "author": info.get('uploader', "YouTubeä½œè€…"),
                "publish_date": info.get('upload_date'),
                "views": info.get('view_count', 0),
                "thumbnail_url": info.get('thumbnail') or f"https://img.youtube.com/vi/{youtube_id}/0.jpg"
            }
    except Exception as e:
        st.error(f"ç²å–YouTubeä¿¡æ¯å¤±æ•—: {str(e)}")
        # è¿”å›é»˜èªå€¼
        return {
            "title": f"YouTubeè¦–é » {youtube_id}",
            "author": "YouTubeä½œè€…",
            "publish_date": None,
            "views": 0,
            "thumbnail_url": f"https://img.youtube.com/vi/{youtube_id}/0.jpg"
        }

def process_document(file, category, tags):
    """è™•ç†ä¸Šå‚³çš„æ–‡æª”"""
    # ç”Ÿæˆå”¯ä¸€ID
    doc_id = str(uuid.uuid4())
    
    # ç²å–æ–‡ä»¶æ“´å±•å
    file_extension = os.path.splitext(file.name)[1].lower()
    
    # ä¿å­˜æ–‡ä»¶
    file_path = f"data/documents/{doc_id}{file_extension}"
    with open(file_path, "wb") as f:
        f.write(file.getbuffer())
    
    # æ ¹æ“šæ–‡ä»¶é¡å‹é¸æ“‡åŠ è¼‰å™¨
    try:
        if file_extension == ".pdf":
            loader = PyPDFLoader(file_path)
        elif file_extension in [".docx", ".doc"]:
            loader = Docx2txtLoader(file_path)
        elif file_extension in [".txt", ".md", ".csv"]:
            loader = TextLoader(file_path)
        else:
            st.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶é¡å‹: {file_extension}")
            return None
        
        # åŠ è¼‰æ–‡æª”
        documents = loader.load()
        
        # æ·»åŠ å…ƒæ•¸æ“š
        valid_documents = []
        for doc in documents:
            if hasattr(doc, 'metadata'):
                doc.metadata["source"] = file.name
                doc.metadata["doc_id"] = doc_id
                doc.metadata["category"] = category
                doc.metadata["tags"] = ",".join(tags) if isinstance(tags, list) else tags
                doc.metadata["date_added"] = datetime.now().isoformat()
                doc.metadata["type"] = "document"
                valid_documents.append(doc)
            else:
                st.warning(f"è·³éç„¡æ•ˆçš„æ–‡æª”: {type(doc)}")
        
        documents = valid_documents
        
        # å°‡æ–‡æª”æ·»åŠ åˆ°æ–‡æª”åˆ—è¡¨
        st.session_state.document_list.append({
            "id": doc_id,
            "name": file.name,
            "type": file_extension[1:],
            "category": category,
            "tags": tags,
            "date_added": datetime.now().isoformat(),
            "path": file_path
        })
        
        return documents
    except Exception as e:
        st.error(f"è™•ç†æ–‡æª”å¤±æ•—: {str(e)}")
        return None

def process_youtube(youtube_url, category, tags):
    """è™•ç†YouTube URL"""
    # æå–YouTube ID
    youtube_id = extract_youtube_id(youtube_url)
    if not youtube_id:
        st.error("ç„¡æ•ˆçš„YouTube URL")
        return None
    
    # ç²å–å­—å¹•
    transcript = get_youtube_transcript(youtube_id)
    if not transcript:
        st.error(f"ç„¡æ³•ç²å–YouTubeè¦–é » {youtube_id} çš„å­—å¹•ï¼Œè«‹ç¢ºä¿è¦–é »æœ‰å­—å¹•")
        return None
    
    # ç²å–YouTubeä¿¡æ¯ï¼ˆå³ä½¿å¤±æ•—ä¹Ÿç¹¼çºŒï¼‰
    info = get_youtube_info(youtube_id)
    
    # ç”Ÿæˆå”¯ä¸€ID
    doc_id = str(uuid.uuid4())
    
    # ä¿å­˜å­—å¹•
    transcript_path = f"data/youtube/{doc_id}.txt"
    with open(transcript_path, "w", encoding="utf-8") as f:
        f.write(transcript)
    
    # å‰µå»ºæ–‡æª”
    loader = TextLoader(transcript_path)
    documents = loader.load()
    
    # æ·»åŠ å…ƒæ•¸æ“š
    valid_documents = []
    for doc in documents:
        if hasattr(doc, 'metadata'):
            doc.metadata["source"] = info["title"]
            doc.metadata["doc_id"] = doc_id
            doc.metadata["youtube_id"] = youtube_id
            doc.metadata["youtube_url"] = youtube_url
            doc.metadata["category"] = category
            doc.metadata["tags"] = ",".join(tags) if isinstance(tags, list) else tags
            doc.metadata["date_added"] = datetime.now().isoformat()
            doc.metadata["type"] = "youtube"
            doc.metadata["author"] = info["author"]
            valid_documents.append(doc)
        else:
            st.warning(f"è·³éç„¡æ•ˆçš„YouTubeæ–‡æª”: {type(doc)}")
    
    documents = valid_documents
    
    # å°‡æ–‡æª”æ·»åŠ åˆ°æ–‡æª”åˆ—è¡¨
    st.session_state.document_list.append({
        "id": doc_id,
        "name": info["title"],
        "type": "youtube",
        "category": category,
        "tags": tags,
        "date_added": datetime.now().isoformat(),
        "path": transcript_path,
        "youtube_id": youtube_id,
        "youtube_url": youtube_url,
        "thumbnail_url": info["thumbnail_url"]
    })
    
    st.success(f"æˆåŠŸæ·»åŠ YouTubeè¦–é »: {info['title']}")
    return documents

def update_vectorstore():
    """æ›´æ–°å‘é‡å­˜å„²"""
    if not st.session_state.openai_api_key:
        st.error("è«‹å…ˆè¨­ç½®OpenAI API Key")
        return
    
    with st.spinner("æ­£åœ¨æ›´æ–°çŸ¥è­˜åº«..."):
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        embeddings = OpenAIEmbeddings()
        
        # å¾æ–‡æª”åˆ—è¡¨ä¸­åŠ è¼‰æ‰€æœ‰æ–‡æª”
        all_documents = []
        for doc_info in st.session_state.document_list:
            try:
                if doc_info["type"] == "youtube":
                    loader = TextLoader(doc_info["path"])
                elif doc_info["type"] == "pdf":
                    loader = PyPDFLoader(doc_info["path"])
                elif doc_info["type"] in ["doc", "docx"]:
                    loader = Docx2txtLoader(doc_info["path"])
                else:
                    loader = TextLoader(doc_info["path"])
                
                documents = loader.load()
                
                # æ·»åŠ å…ƒæ•¸æ“š
                valid_documents = []
                for doc in documents:
                    if hasattr(doc, 'metadata'):
                        doc.metadata["source"] = doc_info["name"]
                        doc.metadata["doc_id"] = doc_info["id"]
                        doc.metadata["category"] = doc_info["category"]
                        doc.metadata["tags"] = ",".join(doc_info["tags"]) if isinstance(doc_info["tags"], list) else doc_info["tags"]
                        doc.metadata["type"] = doc_info["type"]
                        valid_documents.append(doc)
                    else:
                        st.warning(f"è·³éç„¡æ•ˆçš„æ–‡æª”: {type(doc)}")
                
                all_documents.extend(valid_documents)
            except Exception as e:
                st.error(f"åŠ è¼‰æ–‡æª” {doc_info['name']} å¤±æ•—: {str(e)}")
        
        if not all_documents:
            st.warning("æ²’æœ‰å¯ç”¨çš„æ–‡æª”")
            return
        
        # åˆ†å‰²æ–‡æª”
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        chunks = text_splitter.split_documents(all_documents)
        
        # éæ¿¾è¤‡é›œçš„å…ƒæ•¸æ“š
        filtered_chunks = []
        for chunk in chunks:
            # ç¢ºä¿chunkæ˜¯Documentå°è±¡è€Œä¸æ˜¯å­—ç¬¦ä¸²
            if hasattr(chunk, 'metadata'):
                # æ‰‹å‹•éæ¿¾è¤‡é›œçš„å…ƒæ•¸æ“š
                for key in list(chunk.metadata.keys()):
                    value = chunk.metadata[key]
                    # å¦‚æœå€¼æ˜¯è¤‡é›œé¡å‹ï¼ˆä¸æ˜¯strã€intã€floatæˆ–boolï¼‰ï¼Œå‰‡è½‰æ›æˆ–åˆªé™¤
                    if not isinstance(value, (str, int, float, bool)):
                        if isinstance(value, list):
                            # å°‡åˆ—è¡¨è½‰æ›ç‚ºå­—ç¬¦ä¸²
                            chunk.metadata[key] = ",".join(map(str, value))
                        else:
                            # åˆªé™¤å…¶ä»–è¤‡é›œé¡å‹
                            del chunk.metadata[key]
                
                filtered_chunks.append(chunk)
            else:
                st.warning(f"è·³éç„¡æ•ˆçš„æ–‡æª”å¡Š: {type(chunk)}")
        
        # ä½¿ç”¨éæ¿¾å¾Œçš„chunks
        chunks = filtered_chunks
        
        # å‰µå»ºå‘é‡å­˜å„²
        st.session_state.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory="data/vectorstore"
        )
        
        # å‰µå»ºæç¤ºæ¨¡æ¿
        system_template = st.session_state.system_prompt + """

æ–‡æª”å…§å®¹: {context}

è«‹è¨˜ä½ä»¥ä¸‹æ­¥é©Ÿä¾†å›ç­”ç”¨æˆ¶çš„å•é¡Œï¼š
1. ä»”ç´°åˆ†æç”¨æˆ¶å•é¡Œçš„çœŸæ­£æ„åœ–å’Œèªç¾©
2. æ€è€ƒé€™å€‹å•é¡Œåœ¨è²¡å‹™ç¨…æ³•é ˜åŸŸçš„å°ˆæ¥­èƒŒæ™¯å’Œé‡è¦æ€§
3. å¾æä¾›çš„æ–‡æª”ä¸­æ‰¾å‡ºç›¸é—œä¿¡æ¯
4. çµ„ç¹”ä¸€å€‹çµæ§‹åŒ–ã€ç³»çµ±æ€§çš„å›ç­”ï¼Œè€Œä¸åƒ…åƒ…æ˜¯åˆ—å‡ºæ–‡æª”ç‰‡æ®µ
5. å¦‚æœ‰å¿…è¦ï¼Œè£œå……å°ˆæ¥­çŸ¥è­˜ä»¥æä¾›å®Œæ•´å›ç­”
6. ç¢ºä¿å›ç­”ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œå°ˆæ¥­æº–ç¢ºä¸”æ˜“æ–¼ç†è§£
"""
        system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)
        human_template = "{question}"
        human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
        
        # çµ„åˆæç¤ºæ¨¡æ¿
        chat_prompt = ChatPromptTemplate.from_messages([
            system_message_prompt,
            human_message_prompt
        ])
        
        # åˆå§‹åŒ–å°è©±éˆ
        st.session_state.conversation = ConversationalRetrievalChain.from_llm(
            llm=ChatOpenAI(
                temperature=0.7,  # æé«˜æº«åº¦ä»¥ç²å¾—æ›´å¤šæ¨£åŒ–çš„å›ç­”
                model=st.session_state.selected_model  # ä½¿ç”¨ç”¨æˆ¶é¸æ“‡çš„æ¨¡å‹
            ),
            combine_docs_chain_kwargs={"prompt": chat_prompt},  # ä½¿ç”¨è‡ªå®šç¾©æç¤ºæ¨¡æ¿
            retriever=st.session_state.vectorstore.as_retriever(
                search_kwargs={"k": 6}  # æª¢ç´¢6å€‹æœ€ç›¸é—œçš„æ–‡æª”ç‰‡æ®µ
            ),
            memory=ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                output_key="answer"  # æ˜ç¢ºæŒ‡å®šè¦å­˜å„²çš„è¼¸å‡ºéµ
            ),
            chain_type="stuff",  # ä½¿ç”¨stuffæ–¹æ³•å°‡æ‰€æœ‰æª¢ç´¢åˆ°çš„æ–‡æª”åˆä½µåˆ°ä¸€å€‹æç¤ºä¸­
            verbose=True,  # å•Ÿç”¨è©³ç´°æ—¥èªŒ
            return_source_documents=True  # è¿”å›æºæ–‡æª”ä»¥ä¾¿èª¿è©¦
        )
        
        st.success(f"çŸ¥è­˜åº«æ›´æ–°å®Œæˆï¼å…±è™•ç†äº† {len(chunks)} å€‹æ–‡æœ¬å¡Šã€‚")

def process_query(query):
    """è™•ç†ç”¨æˆ¶æŸ¥è©¢"""
    if not st.session_state.conversation:
        st.error("è«‹å…ˆæ›´æ–°çŸ¥è­˜åº«")
        return
    
    with st.spinner("æ€è€ƒä¸­..."):
        response = st.session_state.conversation({"question": query})
        answer = response["answer"]
        source_documents = response.get("source_documents", [])
        
        # æ·»åŠ ä¾†æºä¿¡æ¯
        if source_documents:
            sources_text = "\n\n**åƒè€ƒä¾†æºï¼š**\n"
            
            # å»é‡é‚è¼¯ï¼šä½¿ç”¨é›†åˆè¨˜éŒ„å·²ç¶“æ·»åŠ çš„æ–‡æª”IDå’Œå…§å®¹
            added_sources = set()
            unique_docs = []
            
            for doc in source_documents:
                source = doc.metadata.get("source", "æœªçŸ¥ä¾†æº")
                doc_id = doc.metadata.get("doc_id", "")
                # å‰µå»ºå”¯ä¸€æ¨™è­˜ï¼Œçµåˆæ–‡æª”IDå’Œå…§å®¹çš„å‰50å€‹å­—ç¬¦
                content_hash = f"{doc_id}_{doc.page_content[:50]}"
                
                # å¦‚æœé€™å€‹ä¾†æºé‚„æ²’æœ‰æ·»åŠ éï¼Œå‰‡æ·»åŠ å®ƒ
                if content_hash not in added_sources:
                    added_sources.add(content_hash)
                    unique_docs.append(doc)
            
            # æœ€å¤šé¡¯ç¤º3å€‹ä¸é‡è¤‡çš„ä¾†æº
            for i, doc in enumerate(unique_docs[:3], 1):
                source = doc.metadata.get("source", "æœªçŸ¥ä¾†æº")
                doc_type = doc.metadata.get("type", "æ–‡ä»¶")
                category = doc.metadata.get("category", "")
                
                # æå–æ–‡æª”ç‰‡æ®µçš„ç°¡çŸ­æ‘˜è¦ï¼ˆæœ€å¤š100å€‹å­—ç¬¦ï¼‰
                content_preview = doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
                
                # ç¢ºä¿ä½¿ç”¨ç¹é«”ä¸­æ–‡
                if doc_type == "youtube":
                    sources_text += f"{i}. YouTubeå½±ç‰‡ï¼š{source}\n   ç›¸é—œå…§å®¹ï¼šã€Œ{content_preview}ã€\n"
                else:
                    sources_text += f"{i}. æ–‡ä»¶ï¼š{source} (é¡åˆ¥ï¼š{category})\n   ç›¸é—œå…§å®¹ï¼šã€Œ{content_preview}ã€\n"
            
            # å°‡ä¾†æºä¿¡æ¯æ·»åŠ åˆ°å›ç­”ä¸­
            answer_with_sources = f"{answer}\n{sources_text}"
        else:
            answer_with_sources = answer
        
        st.session_state.chat_history.append({"role": "user", "content": query})
        st.session_state.chat_history.append({"role": "assistant", "content": answer_with_sources})
        
        return answer_with_sources

# çŸ¥è­˜åº«ç®¡ç†é é¢
if page == "çŸ¥è­˜åº«ç®¡ç†":
    st.header("ğŸ“š çŸ¥è­˜åº«ç®¡ç†")
    
    # å‰µå»ºé¸é …å¡
    tab1, tab2, tab3 = st.tabs(["ğŸ“„ æ·»åŠ æ–‡æª”", "ğŸ¬ æ·»åŠ YouTube", "ğŸ—‚ï¸ ç®¡ç†çŸ¥è­˜åº«"])
    
    # æ·»åŠ æ–‡æª”é¸é …å¡
    with tab1:
        st.subheader("ä¸Šå‚³æ–‡æª”")
        
        # æ–‡æª”ä¸Šå‚³è¡¨å–®
        with st.form("document_upload_form"):
            uploaded_file = st.file_uploader("é¸æ“‡æ–‡æª”", type=["pdf", "docx", "txt", "md", "csv"])
            category = st.text_input("åˆ†é¡", "è²¡å‹™ç¨…æ³•")
            tags = st.text_input("æ¨™ç±¤ (ç”¨é€—è™Ÿåˆ†éš”)", "è²¡å‹™,ç¨…æ³•")
            
            submit_button = st.form_submit_button("ä¸Šå‚³æ–‡æª”")
            
            if submit_button and uploaded_file:
                documents = process_document(uploaded_file, category, tags.split(","))
                if documents:
                    st.success(f"æ–‡æª” '{uploaded_file.name}' ä¸Šå‚³æˆåŠŸï¼")
    
    # æ·»åŠ YouTubeé¸é …å¡
    with tab2:
        st.subheader("æ·»åŠ YouTubeå½±ç‰‡")
        
        # YouTube URLè¡¨å–®
        with st.form("youtube_form"):
            youtube_url = st.text_input("YouTube URL")
            category = st.text_input("åˆ†é¡", "è²¡å‹™ç¨…æ³•")
            tags = st.text_input("æ¨™ç±¤ (ç”¨é€—è™Ÿåˆ†éš”)", "è²¡å‹™,ç¨…æ³•")
            
            submit_button = st.form_submit_button("æ·»åŠ YouTube")
            
            if submit_button and youtube_url:
                documents = process_youtube(youtube_url, category, tags.split(","))
                if documents:
                    st.success(f"YouTubeå½±ç‰‡æ·»åŠ æˆåŠŸï¼")
    
    # ç®¡ç†çŸ¥è­˜åº«é¸é …å¡
    with tab3:
        st.subheader("çŸ¥è­˜åº«å…§å®¹")
        
        if st.session_state.document_list:
            # å‰µå»ºæ•¸æ“šæ¡†
            df = pd.DataFrame(st.session_state.document_list)
            df = df[["name", "type", "category", "tags", "date_added"]]
            df.columns = ["åç¨±", "é¡å‹", "åˆ†é¡", "æ¨™ç±¤", "æ·»åŠ æ—¥æœŸ"]
            
            # é¡¯ç¤ºæ•¸æ“šæ¡†
            st.dataframe(df, use_container_width=True)
            
            # åˆªé™¤æ–‡æª”
            if st.button("åˆªé™¤é¸ä¸­çš„æ–‡æª”"):
                st.warning("æ­¤åŠŸèƒ½å°šæœªå¯¦ç¾")
        else:
            st.info("çŸ¥è­˜åº«ä¸­æ²’æœ‰æ–‡æª”")
        
        # æ›´æ–°å‘é‡å­˜å„²
        if st.button("æ›´æ–°çŸ¥è­˜åº«"):
            update_vectorstore()

# èŠå¤©å°è©±é é¢
elif page == "èŠå¤©å°è©±":
    st.header("ğŸ’¬ è²¡å‹™ç¨…æ³•åŠ©æ‰‹")
    
    # æª¢æŸ¥æ˜¯å¦è¨­ç½®äº†APIå¯†é‘°
    if not st.session_state.openai_api_key:
        st.warning("è«‹åœ¨å´é‚Šæ¬„è¨­ç½®OpenAI API Key")
        st.stop()
    
    # æª¢æŸ¥æ˜¯å¦æœ‰æ–‡æª”
    if not st.session_state.document_list:
        st.warning("è«‹å…ˆåœ¨çŸ¥è­˜åº«ç®¡ç†é é¢æ·»åŠ æ–‡æª”")
        st.stop()
    
    # æª¢æŸ¥æ˜¯å¦åˆå§‹åŒ–äº†å‘é‡å­˜å„²
    if not st.session_state.vectorstore:
        st.info("è«‹å…ˆæ›´æ–°çŸ¥è­˜åº«")
        if st.button("æ›´æ–°çŸ¥è­˜åº«"):
            update_vectorstore()
        st.stop()
    
    # é¡¯ç¤ºèŠå¤©æ­·å²
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.write(message["content"])
    
    # èŠå¤©è¼¸å…¥
    user_query = st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ...")
    if user_query:
        # é¡¯ç¤ºç”¨æˆ¶æ¶ˆæ¯
        with st.chat_message("user"):
            st.write(user_query)
        
        # ç²å–å›ç­”
        answer = process_query(user_query)
        
        # é¡¯ç¤ºåŠ©æ‰‹æ¶ˆæ¯
        with st.chat_message("assistant"):
            st.write(answer)

# é è…³
st.sidebar.divider()
st.sidebar.caption("Â© 2025 è²¡å‹™ç¨…æ³•QAæ©Ÿå™¨äºº | ç‰ˆæœ¬ 0.1.0")